{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Transformers Efficient in Production"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Performance Benchmark\n",
    "\n",
    "### Model performance\n",
    "- How well does our model perform on a well-crafted test set that reflects production data? This is especially important when the cost of making errors is large (and best mitigated with a human in the loop), or when we need to run inference on millions of examples and small improvements to the model metrics can translate into large gains in aggregate.\n",
    "\n",
    "### Latency\n",
    "- How fast can our model deliver predictions? We usually care about latency in real-time environments that deal with a lot of traffic, like how Stack Overflow needed a classifier to quickly detect unwelcome comments on the website.\n",
    "\n",
    "### Memory\n",
    "- How can we deploy billion-parameter models like GPT-2 or T5 that require gigabytes of disk storage and RAM? Memory plays an especially important role in mobile or edge devices, where a model has to generate predictions without access to a powerful cloud server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "def compute_size(self):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.compute_size() method\"\"\"\n",
    "    state_dict = self.pipeline.model.state_dict()\n",
    "    tmp_path = Path(\"model.pt\")\n",
    "    torch.save(state_dict, tmp_path)\n",
    "    # Calculate size in megabytes\n",
    "    size_mb = Path(tmp_path).stat().st_size / (1024 * 1024)\n",
    "    # Delete temporary file\n",
    "    tmp_path.unlink()\n",
    "    print(f\"Model size (MB) - {size_mb:.2f}\")\n",
    "    return {\"size_mb\": size_mb}\n",
    "\n",
    "PerformanceBenchmark.compute_size = compute_size\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def time_pipeline(self, query=\"What is the pin number for my account?\"):\n",
    "    \"\"\"This overrides the PerformanceBenchmark.time_pipeline() method\"\"\"\n",
    "    latencies = []\n",
    "    # Warmup\n",
    "    for _ in range(10):\n",
    "        _ = self.pipeline(query)\n",
    "    # Timed run\n",
    "    for _ in range(100):\n",
    "        start_time = perf_counter()\n",
    "        _ = self.pipeline(query)\n",
    "        latency = perf_counter() - start_time\n",
    "        latencies.append(latency)\n",
    "    # Compute run statistics\n",
    "    time_avg_ms = 1000 * np.mean(latencies)\n",
    "    time_std_ms = 1000 * np.std(latencies)\n",
    "    print(f\"Average latency (ms) - {time_avg_ms:.2f} +\\- {time_std_ms:.2f}\")\n",
    "    return {\"time_avg_ms\": time_avg_ms, \"time_std_ms\": time_std_ms}\n",
    "\n",
    "PerformanceBenchmark.time_pipeline = time_pipeline"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
